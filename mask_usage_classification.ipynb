{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5dec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Checks if feature data has been created locally\n",
    "if not os.path.exists(\"mask_data.csv\"):\n",
    "    import featurize\n",
    "    \n",
    "data = np.array(pd.read_csv('mask_data.csv', sep=',',header=None))\n",
    "X, Y = np.split(data, [-1], axis=1)\n",
    "Y = Y.flatten()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019952a8",
   "metadata": {},
   "source": [
    "# PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02f9dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio when mapping to 2047 dimensions is 0.9991406104596937\n",
      "Explained variance ratio when mapping to 1023 dimensions is 0.9944351054826367\n",
      "Explained variance ratio when mapping to 511 dimensions is 0.9838490009086879\n",
      "Explained variance ratio when mapping to 255 dimensions is 0.9661279807652066\n",
      "Explained variance ratio when mapping to 127 dimensions is 0.9387080996549644\n",
      "Explained variance ratio when mapping to 191 dimensions is 0.9560223393412376\n",
      "Explained variance ratio when mapping to 159 dimensions is 0.9486848339439383\n",
      "Explained variance ratio when mapping to 175 dimensions is 0.952603272966209\n",
      "Explained variance ratio when mapping to 167 dimensions is 0.9507209368195808\n",
      "Explained variance ratio when mapping to 163 dimensions is 0.9497079259424271\n",
      "Explained variance ratio when mapping to 165 dimensions is 0.9502179976307089\n",
      "Explained variance ratio when mapping to 164 dimensions is 0.9499773774806068\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=X.shape[1])\n",
    "\n",
    "def binary_search_helper(left, right, X, Y, target):\n",
    "    if left > right:\n",
    "        raise ValueError(\"Binary search helper called with left > right\")\n",
    "    \n",
    "    mid = (left + right) // 2\n",
    "    pca.set_params(**{'n_components':mid})\n",
    "    pca.fit(X, Y) if Y is not None else pca.fit(X)\n",
    "    variance = sum(pca.explained_variance_ratio_)    \n",
    "    print(\"Explained variance ratio when mapping to {} dimensions is {}\".format(\n",
    "        mid, variance\n",
    "    ))\n",
    "        \n",
    "    # Base case\n",
    "    if left == right:\n",
    "        # Use None to denote that we tried exploring left path but it was too low\n",
    "        return left if variance >= target else None \n",
    "    \n",
    "    # Recursive case\n",
    "    if variance == target:\n",
    "        return mid\n",
    "    elif variance >= target:\n",
    "        left_result = binary_search_helper(left, mid - 1, X, Y, target)\n",
    "        return left_result if left_result is not None else mid\n",
    "    else:\n",
    "        return binary_search_helper(mid + 1, right, X, Y, target)\n",
    "        \n",
    "    \n",
    "\n",
    "def required_dimensions(target, X, Y=None) -> int:\n",
    "    \"\"\"\n",
    "    Returns the minimum number of dimensions we can reduce the dataset given by X and Y to \n",
    "    while keeping an explained variance of no less than target.\n",
    "    \"\"\"\n",
    "#     variance = 0\n",
    "#     n_dims = 0\n",
    "#     pca = PCA(n_components=n_dims)\n",
    "#     while variance < target and n_dims < X.shape[0]:\n",
    "#         n_dims += 1\n",
    "#         pca.set_params(**{'n_components':n_dims})\n",
    "#         pca.fit(X, Y) if Y is not None else pca.fit(X)\n",
    "#         variance = sum(pca.explained_variance_ratio_)\n",
    "#         print(\"Explained variance ratio when mapping to {} dimensions is {}\".format(\n",
    "#             n_dims, variance\n",
    "#         ))\n",
    "    return binary_search_helper(0, X.shape[1] - 1, X, Y, target)\n",
    "        \n",
    "\n",
    "# pca = PCA(n_components=100)\n",
    "# pca.fit(X, Y)\n",
    "\n",
    "# print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(required_dimensions(0.95, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389fc68",
   "metadata": {},
   "source": [
    "Thus, we can try mapping the data to $\\mathbb{R}^{165}$ to reduce computational cost later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09ea8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.set_params(**{'n_components':165})\n",
    "pca.fit(X, Y)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4308da",
   "metadata": {},
   "source": [
    "# SVM Classification using various kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a09f6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svm_experiment(kernel : str, C : int, k : int, degree=3):\n",
    "    \"\"\"\n",
    "    Sample code for SVM svc from https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "    \n",
    "    Helper function to run k-fold cross validation on a SVM with a specified kernel.\n",
    "    Returns the trained classifier.\n",
    "    \"\"\"\n",
    "    clf = svm.SVC(kernel=kernel, C=C, random_state=0, degree=degree)\n",
    "    scores = cross_val_score(clf, X, Y, cv=k)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e217851",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_model = svm_experiment(\"linear\", 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b47c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_svm_model = svm_experiment(\"rbf\", 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_svm_model = svm_experiment(\"poly\", 1, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec343525",
   "metadata": {},
   "source": [
    "# XGBoost parameter fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06befbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def xgb_experiment(rounds : int, k : int, param : dict):\n",
    "    \"\"\" \n",
    "    Sample code for xgboost from:\n",
    "    https://xgboost.readthedocs.io/en/latest/python/examples/cross_validation.html\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(X, label=Y)\n",
    "\n",
    "    return xgb.cv(param, dtrain, rounds, nfold=k,\n",
    "           metrics={'merror'}, early_stopping_rounds=10, seed=0,\n",
    "           callbacks=[xgb.callback.EvaluationMonitor(show_stdv=True)])\n",
    "\n",
    "xgb_experiment(10, 5, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\"\"\"\n",
    "Example tuning from \n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\"\"\"\n",
    "\n",
    "param = {\n",
    "    'max_depth':5, \n",
    "    'min_child_weight':1, \n",
    "    'gamma':0,\n",
    "    'eta':2, \n",
    "    'num_class':4,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "}\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        max_depth=5, \n",
    "        min_child_weight=1, \n",
    "        gamma=0,\n",
    "        eta=2, \n",
    "        num_class=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "    ),\n",
    "    param_grid=param_test1,\n",
    ")\n",
    "\n",
    "gsearch1.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bbead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
